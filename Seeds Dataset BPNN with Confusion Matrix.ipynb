{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be009c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b244f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data CSV\n",
    "def load_data():\n",
    "    data = pd.read_csv('seeds_dataset.csv', names=['a','b','c','d','e','f','g','t'])\n",
    "    features = data[['a','b','c','d','e','f','g']]\n",
    "    target = data[['t']]\n",
    "    \n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fc472bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing data\n",
    "def preprocess_data(features, target):\n",
    "    features = MinMaxScaler().fit_transform(features)\n",
    "    target = OneHotEncoder(sparse=False).fit_transform(target)\n",
    "    \n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bbed20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi layers, weights, dan bias\n",
    "layers = {\n",
    "    'input' : 7 ,\n",
    "    'hidden' : 7 , # 2 / 3 dari input + outputnya\n",
    "    'output' : 3,\n",
    "}\n",
    "\n",
    "weights = {\n",
    "    'hidden' : tf.Variable( tf.random.normal( [layers['input'], layers['hidden']] ) ),\n",
    "    'output' : tf.Variable( tf.random.normal( [layers['hidden'], layers['output']] ) ),\n",
    "}\n",
    "\n",
    "bias = {\n",
    "    'hidden' : tf.Variable( tf.random.normal( [layers['hidden'] ])),\n",
    "    'output' : tf.Variable( tf.random.normal( [layers['output'] ])),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e963775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(y):\n",
    "    return tf.nn.sigmoid(y)\n",
    "\n",
    "def feed_forward(features):\n",
    "    Wx1b = tf.matmul(features, weights['hidden']) + bias['hidden']\n",
    "    y1 = activate(Wx1b)\n",
    "\n",
    "    Wx2b = tf.matmul(y1, weights['output']) + bias['output']\n",
    "    y2 = activate(Wx2b)\n",
    "\n",
    "    return y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c51dfd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\radiadus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "features_container = tf.placeholder(tf.float32, [None, layers['input']])\n",
    "target_container = tf.placeholder(tf.float32, [None, layers['output']]) #(?, 3)\n",
    "\n",
    "predicted_target = feed_forward(features_container)\n",
    "error = tf.reduce_mean(0.5 * (target_container - predicted_target)**2 )\n",
    "\n",
    "learning_rate = 0.2\n",
    "epoch = 5000\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate).minimize(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3876d35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         a      b       c      d      e      f      g\n",
      "0    15.26  14.84  0.8710  5.763  3.312  2.221  5.220\n",
      "1    14.88  14.57  0.8811  5.554  3.333  1.018  4.956\n",
      "2    14.29  14.09  0.9050  5.291  3.337  2.699  4.825\n",
      "3    13.84  13.94  0.8955  5.324  3.379  2.259  4.805\n",
      "4    16.14  14.99  0.9034  5.658  3.562  1.355  5.175\n",
      "..     ...    ...     ...    ...    ...    ...    ...\n",
      "205  12.19  13.20  0.8783  5.137  2.981  3.631  4.870\n",
      "206  11.23  12.88  0.8511  5.140  2.795  4.325  5.003\n",
      "207  13.20  13.66  0.8883  5.236  3.232  8.315  5.056\n",
      "208  11.84  13.21  0.8521  5.175  2.836  3.598  5.044\n",
      "209  12.30  13.34  0.8684  5.243  2.974  5.637  5.063\n",
      "\n",
      "[210 rows x 7 columns]\n",
      "     t\n",
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "..  ..\n",
      "205  3\n",
      "206  3\n",
      "207  3\n",
      "208  3\n",
      "209  3\n",
      "\n",
      "[210 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Data Processing\n",
    "features, target = load_data()\n",
    "features, target = preprocess_data(features, target)\n",
    "\n",
    "feature_train, feature_test, target_train, target_test = train_test_split(features, target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f0da515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss Rate: 0.2001960277557373\n",
      "Epoch 500, Loss Rate: 0.09579156339168549\n",
      "Epoch 1000, Loss Rate: 0.07722152024507523\n",
      "Epoch 1500, Loss Rate: 0.06430569291114807\n",
      "Epoch 2000, Loss Rate: 0.05583174154162407\n",
      "Epoch 2500, Loss Rate: 0.049832168966531754\n",
      "Epoch 3000, Loss Rate: 0.0451793372631073\n",
      "Epoch 3500, Loss Rate: 0.04136084392666817\n",
      "Epoch 4000, Loss Rate: 0.038138069212436676\n",
      "Epoch 4500, Loss Rate: 0.035384420305490494\n",
      "Accuracy Row : [ True  True  True  True False  True  True False  True  True  True  True\n",
      " False  True  True  True False  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False  True  True  True False  True\n",
      "  True  True  True  True  True  True]\n",
      "Final Accuracy : 85.71428656578064%\n",
      "Confusion Matrix:\n",
      "[[14  2  2]\n",
      " [ 1  9  0]\n",
      " [ 1  0 13]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(epoch):\n",
    "        train_data = {\n",
    "            features_container: feature_train,\n",
    "            target_container: target_train,\n",
    "        }\n",
    "        sess.run(train, feed_dict = train_data)\n",
    "        loss = sess.run(error, feed_dict = train_data)\n",
    "\n",
    "        if i % 500 ==  0:\n",
    "            print(f'Epoch {i}, Loss Rate: {loss}')\n",
    "\n",
    "    test_data = {\n",
    "        features_container: feature_test,\n",
    "        target_container: target_test,\n",
    "    }\n",
    "\n",
    "    accuracy = tf.equal(tf.argmax(target_container, axis = 1), tf.argmax(predicted_target, axis = 1) )\n",
    "    print(f'Accuracy Row : {sess.run(accuracy, feed_dict = test_data)}')\n",
    "    accuracy = tf.reduce_mean(tf.cast(accuracy, tf.float32))\n",
    "    print(f'Final Accuracy : {sess.run(accuracy, feed_dict = test_data) * 100}%')\n",
    "    conf = tf.math.confusion_matrix(tf.argmax(target_container, axis = 1), tf.argmax(predicted_target, axis = 1) )\n",
    "    print('Confusion Matrix:')\n",
    "    print(sess.run(conf, feed_dict = test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34620cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
